{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n",
    "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. \n",
    "\n",
    "If there is only one independent feature and a dependent feature then ths type of regression in called Simple Linear Regression.\n",
    "\n",
    "Its equation is y = Ho + (H1*x), where as y is dependent variable and x is a independent variable.\n",
    "\n",
    "e.x. Suppose we have a Height-Weight dataset. We have to predict the height based on the weight of a particular person. Here, height is a dependent feature and weight is a independent feature. \n",
    "There is only one independent feature so it is a example if simple linear regression.\n",
    "\n",
    "If there are multiple independent variables then this type of regression is called multiple linear regression.\n",
    "\n",
    "Its equation is y = Ho + H1*X1 + H2*X2 + .... + Hn*Xn\n",
    "\n",
    "e.x. House Price Prediction\n",
    "There are multiple independent features like num of rooms, Location, area, etc using which we predict the price of the house.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "Linearity: You can plot a scatter plot of the dependent variable against each independent variable. If the relationship appears to be linear, this assumption is met.\n",
    "\n",
    "Independence: You can check whether the observations are collected independently of each other. For example, if the data are collected from a time series or a repeated measure design, you need to account for the correlation among the observations.\n",
    "\n",
    "Homoscedasticity: You can plot the residuals against the predicted values. If the residuals appear to be randomly scattered around zero, this assumption is met.\n",
    "\n",
    "Normality: You can plot a histogram of the residuals or a normal probability plot. If the residuals follow a normal distribution, this assumption is met.\n",
    "\n",
    "No multicollinearity: You can calculate the correlation coefficient between each pair of independent variables. If the correlation coefficient is close to 1 or -1, this indicates high multicollinearity.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope represents the rate of change of the dependent variable (Y) with respect to the independent variable (X), while the intercept represents the value of Y when X is equal to zero.\n",
    "\n",
    "\n",
    "For example, consider a real-world scenario where we want to predict the price of a house based on its size. We can use a linear regression model to predict the price (Y) based on the size (X) of the house. Let's say the model we fit is:\n",
    "\n",
    "Price = 100,000 + 200 * Size\n",
    "\n",
    "In this model, the intercept is 100,000, which represents the price of a house with zero size. Of course, this value is not meaningful in the real world, but it is included in the model to allow for the possibility of negative house prices.\n",
    "\n",
    "The slope of the model is 200, which represents the rate of change in the price of a house for every one unit increase in size. In other words, the model suggests that for every additional square foot of space, the price of the house increases by $200.\n",
    "\n",
    "So, if we have a house that is 1,500 square feet in size, we can use the model to predict its price:\n",
    "\n",
    "Price = 100,000 + 200 * 1,500\n",
    "Price = 400,000\n",
    "\n",
    "According to the model, the predicted price of a 1,500 square foot house is $400,000."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "Gradient descent is a commonly used optimization algorithm in machine learning that is used to minimize the cost or loss function of a model. The cost or loss function measures the difference between the predicted output of the model and the actual output. The goal of gradient descent is to find the optimal values of the model parameters that minimize the cost or loss function.\n",
    "\n",
    "The idea behind gradient descent is to iteratively update the model parameters in the opposite direction of the gradient of the cost or loss function with respect to the parameters. This means that we take small steps in the direction of steepest descent until we reach a minimum point, where the gradient of the function is equal to zero.\n",
    "\n",
    "Equation for gradient descent : \n",
    "    Xnew = Xold - alpha * deriative\n",
    "    where alpha is learning rate which is commonly taken as 0.01\n",
    "\n",
    "In other words, gradient descent starts with an initial set of parameter values, and then, in each iteration, it updates the parameters by subtracting the gradient of the cost or loss function with respect to the parameters multiplied by a learning rate, which determines the step size. The learning rate is usually set to a small value to ensure that the algorithm converges to the minimum point and does not overshoot it.\n",
    "\n",
    "The process of updating the parameters continues until the cost or loss function converges to a minimum, indicating that the model has reached a good fit to the data.\n",
    "\n",
    "Gradient descent is used in many machine learning algorithms, including linear regression, logistic regression, and neural networks. It is a key component of deep learning, where it is used to train complex models with millions of parameters. By iteratively adjusting the model parameters, gradient descent allows us to find the best values that make the model most accurate in predicting new unseen data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "\n",
    "Multiple linear regression is a statistical technique used to model the relationship between multiple independent variables (predictors) and a single dependent variable (response). It is an extension of simple linear regression, which only models the relationship between one independent variable and the dependent variable.\n",
    "\n",
    "In multiple linear regression, the model equation takes the form:\n",
    "\n",
    "Y = β0 + β1X1 + β2X2 + ... + βkXk + ε\n",
    "\n",
    "where Y is the dependent variable, β0 is the intercept term, β1, β2, ..., βk are the coefficients or weights that represent the influence of each independent variable X1, X2, ..., Xk on the dependent variable Y, and ε is the error term.\n",
    "\n",
    "The multiple linear regression model estimates the values of the coefficients β1, β2, ..., βk that best fit the observed data by minimizing the sum of squared errors between the predicted values and the actual values. The coefficients can be interpreted as the change in the dependent variable associated with a one-unit change in each independent variable, holding all other independent variables constant.\n",
    "\n",
    "One of the main differences between simple and multiple linear regression is that multiple linear regression allows for the modeling of more complex relationships between the independent variables and the dependent variable. It also allows for the inclusion of multiple predictors, which can improve the accuracy of the model by capturing the joint effect of several variables on the dependent variable. However, it can also make the model more complex and harder to interpret than a simple linear regression model.\n",
    "\n",
    "Another difference is that in multiple linear regression, the coefficient estimates depend on each other, which means that the presence of one predictor can affect the estimated coefficient for another predictor. This is known as the problem of multicollinearity and can lead to unstable coefficient estimates and reduced predictive power. Therefore, it is important to check for multicollinearity and other assumptions of the multiple linear regression model to ensure the validity and accuracy of the results.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "Multicollinearity is a common issue that can arise in multiple linear regression when two or more independent variables (predictors) in the model are highly correlated with each other. This can make it difficult to estimate the independent effects of each predictor on the dependent variable, and can lead to unstable and unreliable coefficient estimates.\n",
    "\n",
    "Multicollinearity can be detected using several methods:\n",
    "\n",
    "Correlation matrix: Calculate the correlation coefficients between each pair of independent variables in the model. If there are high correlations (e.g., above 0.8 or 0.9), then multicollinearity may be present.\n",
    "\n",
    "Variance Inflation Factor (VIF): Calculate the VIF for each independent variable. VIF measures the degree to which the variance of the estimated coefficient for each independent variable is increased due to the presence of multicollinearity. A VIF value of 1 indicates no multicollinearity, while values above 5 or 10 indicate high multicollinearity.\n",
    "\n",
    "Eigenvalues: Calculate the eigenvalues of the correlation matrix. If one or more eigenvalues are close to zero, then multicollinearity may be present.\n",
    "\n",
    "If multicollinearity is detected, there are several ways to address it:\n",
    "\n",
    "Remove one of the correlated predictors: If two or more predictors are highly correlated, remove one of them from the model. This can reduce the multicollinearity and improve the stability of the coefficient estimates.\n",
    "\n",
    "Combine the correlated predictors: Instead of using two or more correlated predictors, combine them into a single variable using a factor analysis or principal component analysis.\n",
    "\n",
    "Regularization: Use regularization methods such as ridge regression or lasso regression, which penalize the magnitude of the coefficient estimates and can reduce the impact of multicollinearity on the model.\n",
    "\n",
    "In summary, multicollinearity is a common issue in multiple linear regression that can lead to unreliable and unstable coefficient estimates. It can be detected using correlation matrices, VIF, and eigenvalues, and can be addressed by removing correlated predictors, combining them into a single variable, or using regularization methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "\n",
    "Polynomial regression is a type of regression analysis used to model the relationship between a dependent variable and one or more independent variables by fitting a polynomial function to the data. In polynomial regression, the model equation takes the form of a polynomial function of degree n:\n",
    "\n",
    "Y = β0 + β1X + β2X^2 + β3X^3 + ... + βnX^n + ε\n",
    "\n",
    "where Y is the dependent variable, X is the independent variable, β0, β1, β2, ..., βn are the coefficients or weights, ε is the error term, and n is the degree of the polynomial function.\n",
    "\n",
    "The main difference between polynomial regression and linear regression is that polynomial regression can capture non-linear relationships between the dependent and independent variables, whereas linear regression assumes a linear relationship between them. In other words, the relationship between the dependent and independent variables is not a straight line in polynomial regression.\n",
    "\n",
    "For example, if the relationship between the dependent variable and independent variable is curvilinear, a polynomial function of degree two (quadratic) can be used to fit the data better than a straight line. Similarly, a cubic or higher degree polynomial function can be used to model more complex non-linear relationships.\n",
    "\n",
    "However, polynomial regression can also lead to overfitting if the degree of the polynomial function is too high. This means that the model fits the training data too well and may not generalize well to new data. Therefore, it is important to balance the complexity of the model with its ability to generalize to new data.\n",
    "\n",
    "In summary, polynomial regression is a type of regression analysis that can model non-linear relationships between the dependent and independent variables by fitting a polynomial function to the data. It differs from linear regression in that it can capture non-linear relationships, but can also lead to overfitting if the degree of the polynomial function is too high.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "Advantages of polynomial regression compared to linear regression:\n",
    "\n",
    "Can model non-linear relationships: Polynomial regression can model non-linear relationships between the dependent and independent variables, while linear regression can only model linear relationships.\n",
    "\n",
    "More flexible: Polynomial regression is more flexible than linear regression since it can fit curves of varying shapes to the data.\n",
    "\n",
    "Can fit complex data: Polynomial regression can fit complex data that cannot be fit by linear regression.\n",
    "\n",
    "Disadvantages of polynomial regression compared to linear regression:\n",
    "\n",
    "More complex: Polynomial regression is more complex than linear regression, which can make it more difficult to interpret the results and may require more data to avoid overfitting.\n",
    "\n",
    "Higher variance: Polynomial regression can have higher variance than linear regression, which can lead to overfitting and poor generalization to new data.\n",
    "\n",
    "Prone to extrapolation errors: Polynomial regression is prone to extrapolation errors, meaning that the model may make inaccurate predictions for values outside the range of the training data.\n",
    "\n",
    "In general, polynomial regression is preferred over linear regression when the relationship between the dependent and independent variables is non-linear and cannot be adequately modeled by a linear function. However, it is important to balance the complexity of the polynomial function with the amount of data available and the risk of overfitting. Polynomial regression can be a useful tool for exploratory analysis, but it is important to validate the model on new data and to consider simpler models if they provide similar performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
